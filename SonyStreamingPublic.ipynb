{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Packages # \n",
    "\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a single number ([-1.0,+1.0]) representing the sentiment of every IMDb user review of the TV show\n",
    "\n",
    "def getSentiment(imdbTag):\n",
    "    html = urlopen('https://www.imdb.com/title/' + imdbTag + '/reviews')\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    reviews = bs.find_all('div',{'class':'text show-more__control'})\n",
    "    compounds = []\n",
    "    for review in reviews:\n",
    "        score = SentimentIntensityAnalyzer().polarity_scores(str(review))['compound']\n",
    "        compounds.append(score)\n",
    "        \n",
    "    if(len(compounds) == 0):\n",
    "        return 0\n",
    "    return sum(compounds)/len(compounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs the sentiment analysis for every row\n",
    "# counter parameter is the row number to start on, 0 to start, but higher later on when the connection is severed\n",
    "\n",
    "def getSentimentForAll(counter):\n",
    "    try:\n",
    "        for i in tqdm_notebook(range(counter,len(linear.index))):\n",
    "            \n",
    "            imdbSentiment = getSentiment(linear['tconst'][i])\n",
    "            linear.iloc[i]['imdbSentiment'] = imdbSentiment\n",
    "            counter += 1\n",
    "    # Catch the severed connection exception, wait 5 minutes, and then resume at the place where it stopped\n",
    "    except ConnectionResetError:\n",
    "        time.sleep(300)\n",
    "        getSentimentForAll(counter)                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in Linear Ratings Data\n",
    "\n",
    "linear = pd.read_csv('/filepath/to/the/data', compression = 'gzip', usecols=['SPT Program Name','Rtg','Daypart'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in IMDB data\n",
    "\n",
    "imdbData = pd.read_csv('/filepath/to/the/data', sep='\\t',usecols=['tconst','primaryTitle','originalTitle'],low_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These need to be strings for later\n",
    "\n",
    "linear['SPT Program Name'] = linear['SPT Program Name'].astype('str')\n",
    "imdbData['primaryTitle'] = imdbData['primaryTitle'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear.groupby(['SPT Program Name'],as_index=False).mean().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdbData.groupby(['primaryTitle'],as_index=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "linear = linear.drop_duplicates(subset='SPT Program Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdbData = imdbData.drop_duplicates(subset='primaryTitle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join the IMDb dataset with the linear TV dataset, every row has an IMDb tag now\n",
    "\n",
    "linear = linear.merge(imdbData, left_on='SPT Program Name', right_on='primaryTitle',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column for IMDb review sentiment to be filled in later\n",
    "\n",
    "linear['imdbSentiment'] = ''\n",
    "cols = ['SPT Program Name','tconst','Rtg','imdbSentiment','Daypart','primaryTitle','originalTitle']\n",
    "linear = linear[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the getSentiment function for every TV show in the dataset. IMDb.com will sever the connection at some point due to the volume of requests\n",
    "\n",
    "counter = 0\n",
    "getSentimentForAll(counter)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to a new .csv\n",
    "linear.to_csv('/filepath/to/put/the/file')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
